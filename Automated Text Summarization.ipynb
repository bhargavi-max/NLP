{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b64931",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"In a world often dominated by negativity, it's important to remember the power of kindness and compassion. Small acts of kindness have the ability to brighten someone's day, uplift spirits, and create a ripple effect of positivity that can spread far and wide. Whether it's a smile to a stranger, a helping hand to a friend in need, or a thoughtful gesture to a colleague, every act of kindness has the potential to make a difference in someone's life.Beyond individual actions, there is also immense power in collective efforts to create positive change. When communities come together to support one another, incredible things can happen. From grassroots initiatives to global movements, people are uniting to tackle pressing social and environmental issues, driving meaningful progress and inspiring hope for a better future.It's also important to recognize the strength that lies within each and every one of us. We all have the ability to make a positive impact, no matter how small our actions may seem. By tapping into our innate compassion and empathy, we can cultivate a culture of kindness and empathy that enriches our lives and those around us.So let's embrace the power of kindness, and strive to make the world a better place one small act at a time. Together, we can create a brighter, more compassionate future for all.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c2cdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1335"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ddbf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6fc5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83a2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d47676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.text.lower() for token in doc \n",
    "          if not token.is_stop and \n",
    "          not token.is_punct and \n",
    "          token.text !='\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2d4a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world',\n",
       " 'dominated',\n",
       " 'negativity',\n",
       " 'important',\n",
       " 'remember',\n",
       " 'power',\n",
       " 'kindness',\n",
       " 'compassion',\n",
       " 'small',\n",
       " 'acts',\n",
       " 'kindness',\n",
       " 'ability',\n",
       " 'brighten',\n",
       " 'day',\n",
       " 'uplift',\n",
       " 'spirits',\n",
       " 'create',\n",
       " 'ripple',\n",
       " 'effect',\n",
       " 'positivity',\n",
       " 'spread',\n",
       " 'far',\n",
       " 'wide',\n",
       " 'smile',\n",
       " 'stranger',\n",
       " 'helping',\n",
       " 'hand',\n",
       " 'friend',\n",
       " 'need',\n",
       " 'thoughtful',\n",
       " 'gesture',\n",
       " 'colleague',\n",
       " 'act',\n",
       " 'kindness',\n",
       " 'potential',\n",
       " 'difference',\n",
       " 'life',\n",
       " 'individual',\n",
       " 'actions',\n",
       " 'immense',\n",
       " 'power',\n",
       " 'collective',\n",
       " 'efforts',\n",
       " 'create',\n",
       " 'positive',\n",
       " 'change',\n",
       " 'communities',\n",
       " 'come',\n",
       " 'support',\n",
       " 'incredible',\n",
       " 'things',\n",
       " 'happen',\n",
       " 'grassroots',\n",
       " 'initiatives',\n",
       " 'global',\n",
       " 'movements',\n",
       " 'people',\n",
       " 'uniting',\n",
       " 'tackle',\n",
       " 'pressing',\n",
       " 'social',\n",
       " 'environmental',\n",
       " 'issues',\n",
       " 'driving',\n",
       " 'meaningful',\n",
       " 'progress',\n",
       " 'inspiring',\n",
       " 'hope',\n",
       " 'better',\n",
       " 'future',\n",
       " 'important',\n",
       " 'recognize',\n",
       " 'strength',\n",
       " 'lies',\n",
       " 'ability',\n",
       " 'positive',\n",
       " 'impact',\n",
       " 'matter',\n",
       " 'small',\n",
       " 'actions',\n",
       " 'tapping',\n",
       " 'innate',\n",
       " 'compassion',\n",
       " 'empathy',\n",
       " 'cultivate',\n",
       " 'culture',\n",
       " 'kindness',\n",
       " 'empathy',\n",
       " 'enriches',\n",
       " 'lives',\n",
       " 'let',\n",
       " 'embrace',\n",
       " 'power',\n",
       " 'kindness',\n",
       " 'strive',\n",
       " 'world',\n",
       " 'better',\n",
       " 'place',\n",
       " 'small',\n",
       " 'act',\n",
       " 'time',\n",
       " 'create',\n",
       " 'brighter',\n",
       " 'compassionate',\n",
       " 'future']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df40f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative approach to remove stop words, Punc\n",
    "tokens1=[]\n",
    "stopwords = list(STOP_WORDS)\n",
    "allowed_pos = ['ADJ','PROPN','VERB','NOUN']\n",
    "for token in doc:\n",
    "    if token.text in stopwords or token.text in punctuation:\n",
    "        continue\n",
    "    if token.pos_ in allowed_pos:\n",
    "        tokens1.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051d4352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world',\n",
       " 'dominated',\n",
       " 'negativity',\n",
       " 'important',\n",
       " 'remember',\n",
       " 'power',\n",
       " 'kindness',\n",
       " 'compassion',\n",
       " 'Small',\n",
       " 'acts',\n",
       " 'kindness',\n",
       " 'ability',\n",
       " 'brighten',\n",
       " 'day',\n",
       " 'uplift',\n",
       " 'spirits',\n",
       " 'create',\n",
       " 'ripple',\n",
       " 'effect',\n",
       " 'positivity',\n",
       " 'spread',\n",
       " 'wide',\n",
       " 'smile',\n",
       " 'stranger',\n",
       " 'helping',\n",
       " 'hand',\n",
       " 'friend',\n",
       " 'need',\n",
       " 'thoughtful',\n",
       " 'gesture',\n",
       " 'colleague',\n",
       " 'act',\n",
       " 'kindness',\n",
       " 'potential',\n",
       " 'difference',\n",
       " 'life',\n",
       " 'individual',\n",
       " 'actions',\n",
       " 'immense',\n",
       " 'power',\n",
       " 'collective',\n",
       " 'efforts',\n",
       " 'create',\n",
       " 'positive',\n",
       " 'change',\n",
       " 'communities',\n",
       " 'come',\n",
       " 'support',\n",
       " 'incredible',\n",
       " 'things',\n",
       " 'happen',\n",
       " 'grassroots',\n",
       " 'initiatives',\n",
       " 'global',\n",
       " 'movements',\n",
       " 'people',\n",
       " 'uniting',\n",
       " 'tackle',\n",
       " 'pressing',\n",
       " 'social',\n",
       " 'environmental',\n",
       " 'issues',\n",
       " 'driving',\n",
       " 'meaningful',\n",
       " 'progress',\n",
       " 'inspiring',\n",
       " 'hope',\n",
       " 'better',\n",
       " 'future',\n",
       " 'important',\n",
       " 'recognize',\n",
       " 'strength',\n",
       " 'lies',\n",
       " 'ability',\n",
       " 'positive',\n",
       " 'impact',\n",
       " 'small',\n",
       " 'actions',\n",
       " 'tapping',\n",
       " 'innate',\n",
       " 'compassion',\n",
       " 'empathy',\n",
       " 'cultivate',\n",
       " 'culture',\n",
       " 'kindness',\n",
       " 'empathy',\n",
       " 'enriches',\n",
       " 'lives',\n",
       " 'let',\n",
       " 'embrace',\n",
       " 'power',\n",
       " 'kindness',\n",
       " 'strive',\n",
       " 'world',\n",
       " 'better',\n",
       " 'place',\n",
       " 'small',\n",
       " 'act',\n",
       " 'time',\n",
       " 'create',\n",
       " 'brighter',\n",
       " 'compassionate',\n",
       " 'future']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f8e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fdaa451",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a94501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'world': 2,\n",
       "         'dominated': 1,\n",
       "         'negativity': 1,\n",
       "         'important': 2,\n",
       "         'remember': 1,\n",
       "         'power': 3,\n",
       "         'kindness': 5,\n",
       "         'compassion': 2,\n",
       "         'small': 3,\n",
       "         'acts': 1,\n",
       "         'ability': 2,\n",
       "         'brighten': 1,\n",
       "         'day': 1,\n",
       "         'uplift': 1,\n",
       "         'spirits': 1,\n",
       "         'create': 3,\n",
       "         'ripple': 1,\n",
       "         'effect': 1,\n",
       "         'positivity': 1,\n",
       "         'spread': 1,\n",
       "         'far': 1,\n",
       "         'wide': 1,\n",
       "         'smile': 1,\n",
       "         'stranger': 1,\n",
       "         'helping': 1,\n",
       "         'hand': 1,\n",
       "         'friend': 1,\n",
       "         'need': 1,\n",
       "         'thoughtful': 1,\n",
       "         'gesture': 1,\n",
       "         'colleague': 1,\n",
       "         'act': 2,\n",
       "         'potential': 1,\n",
       "         'difference': 1,\n",
       "         'life': 1,\n",
       "         'individual': 1,\n",
       "         'actions': 2,\n",
       "         'immense': 1,\n",
       "         'collective': 1,\n",
       "         'efforts': 1,\n",
       "         'positive': 2,\n",
       "         'change': 1,\n",
       "         'communities': 1,\n",
       "         'come': 1,\n",
       "         'support': 1,\n",
       "         'incredible': 1,\n",
       "         'things': 1,\n",
       "         'happen': 1,\n",
       "         'grassroots': 1,\n",
       "         'initiatives': 1,\n",
       "         'global': 1,\n",
       "         'movements': 1,\n",
       "         'people': 1,\n",
       "         'uniting': 1,\n",
       "         'tackle': 1,\n",
       "         'pressing': 1,\n",
       "         'social': 1,\n",
       "         'environmental': 1,\n",
       "         'issues': 1,\n",
       "         'driving': 1,\n",
       "         'meaningful': 1,\n",
       "         'progress': 1,\n",
       "         'inspiring': 1,\n",
       "         'hope': 1,\n",
       "         'better': 2,\n",
       "         'future': 2,\n",
       "         'recognize': 1,\n",
       "         'strength': 1,\n",
       "         'lies': 1,\n",
       "         'impact': 1,\n",
       "         'matter': 1,\n",
       "         'tapping': 1,\n",
       "         'innate': 1,\n",
       "         'empathy': 2,\n",
       "         'cultivate': 1,\n",
       "         'culture': 1,\n",
       "         'enriches': 1,\n",
       "         'lives': 1,\n",
       "         'let': 1,\n",
       "         'embrace': 1,\n",
       "         'strive': 1,\n",
       "         'place': 1,\n",
       "         'time': 1,\n",
       "         'brighter': 1,\n",
       "         'compassionate': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a73e0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq = max(word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7f6a97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70e13ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize values\n",
    "for word in word_freq.keys():\n",
    "    word_freq[word] = word_freq[word]/max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecaac36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'world': 0.4,\n",
       "         'dominated': 0.2,\n",
       "         'negativity': 0.2,\n",
       "         'important': 0.4,\n",
       "         'remember': 0.2,\n",
       "         'power': 0.6,\n",
       "         'kindness': 1.0,\n",
       "         'compassion': 0.4,\n",
       "         'small': 0.6,\n",
       "         'acts': 0.2,\n",
       "         'ability': 0.4,\n",
       "         'brighten': 0.2,\n",
       "         'day': 0.2,\n",
       "         'uplift': 0.2,\n",
       "         'spirits': 0.2,\n",
       "         'create': 0.6,\n",
       "         'ripple': 0.2,\n",
       "         'effect': 0.2,\n",
       "         'positivity': 0.2,\n",
       "         'spread': 0.2,\n",
       "         'far': 0.2,\n",
       "         'wide': 0.2,\n",
       "         'smile': 0.2,\n",
       "         'stranger': 0.2,\n",
       "         'helping': 0.2,\n",
       "         'hand': 0.2,\n",
       "         'friend': 0.2,\n",
       "         'need': 0.2,\n",
       "         'thoughtful': 0.2,\n",
       "         'gesture': 0.2,\n",
       "         'colleague': 0.2,\n",
       "         'act': 0.4,\n",
       "         'potential': 0.2,\n",
       "         'difference': 0.2,\n",
       "         'life': 0.2,\n",
       "         'individual': 0.2,\n",
       "         'actions': 0.4,\n",
       "         'immense': 0.2,\n",
       "         'collective': 0.2,\n",
       "         'efforts': 0.2,\n",
       "         'positive': 0.4,\n",
       "         'change': 0.2,\n",
       "         'communities': 0.2,\n",
       "         'come': 0.2,\n",
       "         'support': 0.2,\n",
       "         'incredible': 0.2,\n",
       "         'things': 0.2,\n",
       "         'happen': 0.2,\n",
       "         'grassroots': 0.2,\n",
       "         'initiatives': 0.2,\n",
       "         'global': 0.2,\n",
       "         'movements': 0.2,\n",
       "         'people': 0.2,\n",
       "         'uniting': 0.2,\n",
       "         'tackle': 0.2,\n",
       "         'pressing': 0.2,\n",
       "         'social': 0.2,\n",
       "         'environmental': 0.2,\n",
       "         'issues': 0.2,\n",
       "         'driving': 0.2,\n",
       "         'meaningful': 0.2,\n",
       "         'progress': 0.2,\n",
       "         'inspiring': 0.2,\n",
       "         'hope': 0.2,\n",
       "         'better': 0.4,\n",
       "         'future': 0.4,\n",
       "         'recognize': 0.2,\n",
       "         'strength': 0.2,\n",
       "         'lies': 0.2,\n",
       "         'impact': 0.2,\n",
       "         'matter': 0.2,\n",
       "         'tapping': 0.2,\n",
       "         'innate': 0.2,\n",
       "         'empathy': 0.4,\n",
       "         'cultivate': 0.2,\n",
       "         'culture': 0.2,\n",
       "         'enriches': 0.2,\n",
       "         'lives': 0.2,\n",
       "         'let': 0.2,\n",
       "         'embrace': 0.2,\n",
       "         'strive': 0.2,\n",
       "         'place': 0.2,\n",
       "         'time': 0.2,\n",
       "         'brighter': 0.2,\n",
       "         'compassionate': 0.2})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b1a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence tokenization\n",
    "sent_token = [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d4392ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In a world often dominated by negativity, it's important to remember the power of kindness and compassion.\",\n",
       " \"Small acts of kindness have the ability to brighten someone's day, uplift spirits, and create a ripple effect of positivity that can spread far and wide.\",\n",
       " \"Whether it's a smile to a stranger, a helping hand to a friend in need, or a thoughtful gesture to a colleague, every act of kindness has the potential to make a difference in someone's life.\",\n",
       " 'Beyond individual actions, there is also immense power in collective efforts to create positive change.',\n",
       " 'When communities come together to support one another, incredible things can happen.',\n",
       " 'From grassroots initiatives to global movements, people are uniting to tackle pressing social and environmental issues, driving meaningful progress and inspiring hope for a better future.',\n",
       " \"It's also important to recognize the strength that lies within each and every one of us.\",\n",
       " 'We all have the ability to make a positive impact, no matter how small our actions may seem.',\n",
       " 'By tapping into our innate compassion and empathy, we can cultivate a culture of kindness and empathy that enriches our lives and those around us.',\n",
       " \"So let's embrace the power of kindness, and strive to make the world a better place one small act at a time.\",\n",
       " 'Together, we can create a brighter, more compassionate future for all.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8b9cac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "a\n",
      "world\n",
      "often\n",
      "dominated\n",
      "by\n",
      "negativity,\n",
      "it's\n",
      "important\n",
      "to\n",
      "remember\n",
      "the\n",
      "power\n",
      "of\n",
      "kindness\n",
      "and\n",
      "compassion.\n",
      "Small\n",
      "acts\n",
      "of\n",
      "kindness\n",
      "have\n",
      "the\n",
      "ability\n",
      "to\n",
      "brighten\n",
      "someone's\n",
      "day,\n",
      "uplift\n",
      "spirits,\n",
      "and\n",
      "create\n",
      "a\n",
      "ripple\n",
      "effect\n",
      "of\n",
      "positivity\n",
      "that\n",
      "can\n",
      "spread\n",
      "far\n",
      "and\n",
      "wide.\n",
      "Whether\n",
      "it's\n",
      "a\n",
      "smile\n",
      "to\n",
      "a\n",
      "stranger,\n",
      "a\n",
      "helping\n",
      "hand\n",
      "to\n",
      "a\n",
      "friend\n",
      "in\n",
      "need,\n",
      "or\n",
      "a\n",
      "thoughtful\n",
      "gesture\n",
      "to\n",
      "a\n",
      "colleague,\n",
      "every\n",
      "act\n",
      "of\n",
      "kindness\n",
      "has\n",
      "the\n",
      "potential\n",
      "to\n",
      "make\n",
      "a\n",
      "difference\n",
      "in\n",
      "someone's\n",
      "life.\n",
      "Beyond\n",
      "individual\n",
      "actions,\n",
      "there\n",
      "is\n",
      "also\n",
      "immense\n",
      "power\n",
      "in\n",
      "collective\n",
      "efforts\n",
      "to\n",
      "create\n",
      "positive\n",
      "change.\n",
      "When\n",
      "communities\n",
      "come\n",
      "together\n",
      "to\n",
      "support\n",
      "one\n",
      "another,\n",
      "incredible\n",
      "things\n",
      "can\n",
      "happen.\n",
      "From\n",
      "grassroots\n",
      "initiatives\n",
      "to\n",
      "global\n",
      "movements,\n",
      "people\n",
      "are\n",
      "uniting\n",
      "to\n",
      "tackle\n",
      "pressing\n",
      "social\n",
      "and\n",
      "environmental\n",
      "issues,\n",
      "driving\n",
      "meaningful\n",
      "progress\n",
      "and\n",
      "inspiring\n",
      "hope\n",
      "for\n",
      "a\n",
      "better\n",
      "future.\n",
      "It's\n",
      "also\n",
      "important\n",
      "to\n",
      "recognize\n",
      "the\n",
      "strength\n",
      "that\n",
      "lies\n",
      "within\n",
      "each\n",
      "and\n",
      "every\n",
      "one\n",
      "of\n",
      "us.\n",
      "We\n",
      "all\n",
      "have\n",
      "the\n",
      "ability\n",
      "to\n",
      "make\n",
      "a\n",
      "positive\n",
      "impact,\n",
      "no\n",
      "matter\n",
      "how\n",
      "small\n",
      "our\n",
      "actions\n",
      "may\n",
      "seem.\n",
      "By\n",
      "tapping\n",
      "into\n",
      "our\n",
      "innate\n",
      "compassion\n",
      "and\n",
      "empathy,\n",
      "we\n",
      "can\n",
      "cultivate\n",
      "a\n",
      "culture\n",
      "of\n",
      "kindness\n",
      "and\n",
      "empathy\n",
      "that\n",
      "enriches\n",
      "our\n",
      "lives\n",
      "and\n",
      "those\n",
      "around\n",
      "us.\n",
      "So\n",
      "let's\n",
      "embrace\n",
      "the\n",
      "power\n",
      "of\n",
      "kindness,\n",
      "and\n",
      "strive\n",
      "to\n",
      "make\n",
      "the\n",
      "world\n",
      "a\n",
      "better\n",
      "place\n",
      "one\n",
      "small\n",
      "act\n",
      "at\n",
      "a\n",
      "time.\n",
      "Together,\n",
      "we\n",
      "can\n",
      "create\n",
      "a\n",
      "brighter,\n",
      "more\n",
      "compassionate\n",
      "future\n",
      "for\n",
      "all.\n"
     ]
    }
   ],
   "source": [
    "#store scores of each sentence\n",
    "sent_score = {}\n",
    "for sent in sent_token:\n",
    "    for word in sent.split():\n",
    "        if word.lower() in word_freq.keys():\n",
    "            if sent not in sent_score.keys():\n",
    "                sent_score[sent] = word_freq[word]\n",
    "            else:\n",
    "                sent_score[sent] +=word_freq[word]\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30501c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"In a world often dominated by negativity, it's important to remember the power of kindness and compassion.\": 2.8,\n",
       " \"Small acts of kindness have the ability to brighten someone's day, uplift spirits, and create a ripple effect of positivity that can spread far and wide.\": 3.600000000000001,\n",
       " \"Whether it's a smile to a stranger, a helping hand to a friend in need, or a thoughtful gesture to a colleague, every act of kindness has the potential to make a difference in someone's life.\": 3.0000000000000004,\n",
       " 'Beyond individual actions, there is also immense power in collective efforts to create positive change.': 2.4,\n",
       " 'When communities come together to support one another, incredible things can happen.': 1.0,\n",
       " 'From grassroots initiatives to global movements, people are uniting to tackle pressing social and environmental issues, driving meaningful progress and inspiring hope for a better future.': 3.2,\n",
       " \"It's also important to recognize the strength that lies within each and every one of us.\": 1.0,\n",
       " 'We all have the ability to make a positive impact, no matter how small our actions may seem.': 2.0,\n",
       " 'By tapping into our innate compassion and empathy, we can cultivate a culture of kindness and empathy that enriches our lives and those around us.': 3.0000000000000004,\n",
       " \"So let's embrace the power of kindness, and strive to make the world a better place one small act at a time.\": 2.9999999999999996,\n",
       " 'Together, we can create a brighter, more compassionate future for all.': 1.2000000000000002}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23917093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87e5220d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In a world often dominated by negativity, it's...</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Small acts of kindness have the ability to bri...</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whether it's a smile to a stranger, a helping ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beyond individual actions, there is also immen...</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When communities come together to support one ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From grassroots initiatives to global movement...</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It's also important to recognize the strength ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We all have the ability to make a positive imp...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>By tapping into our innate compassion and empa...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>So let's embrace the power of kindness, and st...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Together, we can create a brighter, more compa...</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  Score\n",
       "0   In a world often dominated by negativity, it's...    2.8\n",
       "1   Small acts of kindness have the ability to bri...    3.6\n",
       "2   Whether it's a smile to a stranger, a helping ...    3.0\n",
       "3   Beyond individual actions, there is also immen...    2.4\n",
       "4   When communities come together to support one ...    1.0\n",
       "5   From grassroots initiatives to global movement...    3.2\n",
       "6   It's also important to recognize the strength ...    1.0\n",
       "7   We all have the ability to make a positive imp...    2.0\n",
       "8   By tapping into our innate compassion and empa...    3.0\n",
       "9   So let's embrace the power of kindness, and st...    3.0\n",
       "10  Together, we can create a brighter, more compa...    1.2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(sent_score.items()),columns=['Sentence','Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94fb345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a1af840",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences =3\n",
    "n = nlargest(num_sentences,sent_score,key=sent_score.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f83db739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Small acts of kindness have the ability to brighten someone's day, uplift spirits, and create a ripple effect of positivity that can spread far and wide. From grassroots initiatives to global movements, people are uniting to tackle pressing social and environmental issues, driving meaningful progress and inspiring hope for a better future. Whether it's a smile to a stranger, a helping hand to a friend in need, or a thoughtful gesture to a colleague, every act of kindness has the potential to make a difference in someone's life.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2891d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences =1\n",
    "n = nlargest(num_sentences,sent_score,key=sent_score.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d062e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Small acts of kindness have the ability to brighten someone's day, uplift spirits, and create a ripple effect of positivity that can spread far and wide.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e364e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement Tkinter (from versions: none)\n",
      "ERROR: No matching distribution found for Tkinter\n"
     ]
    }
   ],
   "source": [
    "!pip install Tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ad35ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from tkinter import messagebox\n",
    "from tkinter import END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4420b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the summarization code\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "\n",
    "def summarize_text():\n",
    "    # Get text from the text box\n",
    "    text = text_box.get(\"1.0\", \"end-1c\")\n",
    "    \n",
    "    # Loading spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Tokenization and removing stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.lower() for token in doc \n",
    "              if not token.is_stop and not token.is_punct and token.text != '\\n']\n",
    "\n",
    "    # Calculating word frequency\n",
    "    word_freq = Counter(tokens)\n",
    "    if not word_freq:\n",
    "        messagebox.showerror(\"Error\", \"No words found in the text.\")\n",
    "        return\n",
    "    \n",
    "    max_freq = max(word_freq.values())\n",
    "    for word in word_freq.keys():\n",
    "        word_freq[word] = word_freq[word]/max_freq\n",
    "    \n",
    "    # Sentence tokenization\n",
    "    sent_token = [sent.text for sent in doc.sents]\n",
    "\n",
    "    sent_score = {}\n",
    "    for sent in sent_token:\n",
    "        for word in sent.split():\n",
    "            if word.lower() in word_freq.keys():\n",
    "                if sent not in sent_score.keys():\n",
    "                    sent_score[sent] = word_freq[word]\n",
    "                else:\n",
    "                    sent_score[sent] += word_freq[word]\n",
    "\n",
    "    # Select top-scoring sentences based on user input\n",
    "    num_sentences = int(num_sentences_entry.get())\n",
    "    summarized_sentences = nlargest(num_sentences, sent_score, key=sent_score.get)\n",
    "\n",
    "    # Display summarized text in the result box\n",
    "    result_box.delete(1.0, END)\n",
    "    result_box.insert(END, \" \".join(summarized_sentences))\n",
    "\n",
    "# GUI setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Text Summarizer\")\n",
    "\n",
    "# Text box for input\n",
    "text_box = scrolledtext.ScrolledText(root, width=50, height=10, wrap=tk.WORD)\n",
    "text_box.pack(pady=10)\n",
    "\n",
    "# Entry field for the number of sentences\n",
    "num_sentences_label = tk.Label(root, text=\"Number of Sentences:\")\n",
    "num_sentences_label.pack()\n",
    "num_sentences_entry = tk.Entry(root, width=10)\n",
    "num_sentences_entry.insert(END, \"3\")  # Default value\n",
    "num_sentences_entry.pack()\n",
    "\n",
    "# Button to summarize\n",
    "summarize_button = tk.Button(root, text=\"Summarize\", command=summarize_text)\n",
    "summarize_button.pack(pady=5)\n",
    "\n",
    "# Result box for output\n",
    "result_box = scrolledtext.ScrolledText(root, width=50, height=5, wrap=tk.WORD)\n",
    "result_box.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01639dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#Abstractive text summarization\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f77c36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (5.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "  WARNING: The scripts accelerate-config.exe, accelerate-estimate-memory.exe, accelerate-launch.exe, accelerate-merge-weights.exe and accelerate.exe are installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (0.24.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.4)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.10)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n",
      "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.34.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3412f8a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0x10 but this version of numpy is 0xf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0x10 but this version of numpy is 0xf"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\ninitialization of _pywrap_checkpoint_reader raised unreported exception",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py:1603\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\pipelines\\__init__.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\image_processing_utils.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\image_transforms.py:49\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\__init__.py:47\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:205\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:34\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debug_mode\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iterator_ops\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m options \u001b[38;5;28;01mas\u001b[39;00m options_lib\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:45\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base \u001b[38;5;28;01mas\u001b[39;00m trackable\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSaverBuilder\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecation\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\training\\saver.py:50\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base \u001b[38;5;28;01mas\u001b[39;00m trackable\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_checkpoint_reader\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_util\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_checkpoint_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckpointReader\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n",
      "\u001b[1;31mSystemError\u001b[0m: initialization of _pywrap_checkpoint_reader raised unreported exception",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py:1593\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1591\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1593\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1594\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py:1605\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1608\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\ninitialization of _pywrap_checkpoint_reader raised unreported exception"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f56a5fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (4.44.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.0-cp39-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 9.9/9.9 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.0-cp39-none-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 2.3/2.3 MB 4.9 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.2\n",
      "    Uninstalling transformers-4.44.2:\n",
      "      Successfully uninstalled transformers-4.44.2\n",
      "Successfully installed tokenizers-0.20.0 transformers-4.45.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers tensorflow --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b087e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\admin\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\tensorflow\\\\python\\\\_pywrap_dtensor_device.pyd'\n",
      "Check the permissions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting transformers==4.26.0\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl.metadata (100 kB)\n",
      "Collecting tensorflow==2.9.0\n",
      "  Downloading tensorflow-2.9.0-cp39-cp39-win_amd64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.26.0) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.26.0) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from transformers==4.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.26.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.26.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.26.0) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.26.0) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.0)\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.26.0) (4.64.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (1.6.3)\n",
      "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.0)\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.0)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (3.11.0)\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.0)\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.9.0) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow==2.9.0) (1.62.2)\n",
      "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0)\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.9.0) (0.37.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (2024.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers==4.26.0) (3.0.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (1.33.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.3.4)\n",
      "Collecting protobuf>=3.9.2 (from tensorflow==2.9.0)\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl.metadata (807 bytes)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.0.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.26.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.26.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.26.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers==4.26.0) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers==4.26.0) (0.4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.9.0-cp39-cp39-win_amd64.whl (444.0 MB)\n",
      "   ---------------------------------------- 444.0/444.0 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 1.6/1.6 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "   ---------------------------------------- 5.8/5.8 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "   ---------------------------------------- 895.9/895.9 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Downloading tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 3.5/3.5 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "   ---------------------------------------- 781.3/781.3 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tokenizers, tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, oauthlib, keras-preprocessing, gast, requests-oauthlib, transformers, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.0\n",
      "    Uninstalling tokenizers-0.20.0:\n",
      "      Successfully uninstalled tokenizers-0.20.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.2.1\n",
      "    Uninstalling keras-3.2.1:\n",
      "      Successfully uninstalled keras-3.2.1\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 24.3.25\n",
      "    Uninstalling flatbuffers-24.3.25:\n",
      "      Successfully uninstalled flatbuffers-24.3.25\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.4\n",
      "    Uninstalling gast-0.5.4:\n",
      "      Successfully uninstalled gast-0.5.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "  Rolling back uninstall of tensorflow\n",
      "  Moving to c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages\\tensorflow-2.17.0.dist-info\\\n",
      "   from C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\~ensorflow-2.17.0.dist-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.26.0 tensorflow==2.9.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e207777",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587df274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba50c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4646a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75a67b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.27.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.12.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.4 which is incompatible.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.31.0)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: rich in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.10)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\n",
      "Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 15.8/15.8 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93426fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"t5-base\"\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)  # TensorFlow model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the summarizer pipeline\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework='tf')  # Set framework to 'tf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2566d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"In a world often dominated by negativity, it's important to remember the power of kindness and compassion. Small acts of kindness have the ability to brighten someone's day, uplift spirits, and create a ripple effect of positivity that can spread far and wide. Whether it's a smile to a stranger, a helping hand to a friend in need, or a thoughtful gesture to a colleague, every act of kindness has the potential to make a difference in someone's life.Beyond individual actions, there is also immense power in collective efforts to create positive change. When communities come together to support one another, incredible things can happen. From grassroots initiatives to global movements, people are uniting to tackle pressing social and environmental issues, driving meaningful progress and inspiring hope for a better future.It's also important to recognize the strength that lies within each and every one of us. We all have the ability to make a positive impact, no matter how small our actions may seem. By tapping into our innate compassion and empathy, we can cultivate a culture of kindness and empathy that enriches our lives and those around us.So let's embrace the power of kindness, and strive to make the world a better place one small act at a time. Together, we can create a brighter, more compassionate future for all.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e38498",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarizer(text,max_length=100,min_length=10,do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e674df",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d740a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c01eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text():\n",
    "    # Get text from the input text box\n",
    "    text = text_entry.get(\"1.0\", \"end-1c\")\n",
    "\n",
    "    # Summarize the text\n",
    "    summary = summarizer(text, max_length=100, min_length=10, do_sample=False)\n",
    "\n",
    "    # Update the output text box with the summary\n",
    "    output_text.delete(\"1.0\", \"end\")\n",
    "    output_text.insert(\"1.0\", summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d914ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "window.title(\"Text Summarizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72021fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input text box\n",
    "text_entry = tk.Text(window, height=10, width=60)\n",
    "text_entry.pack(pady=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab70916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a button to trigger text summarization\n",
    "summarize_button = tk.Button(window, text=\"Summarize\", command=summarize_text)\n",
    "summarize_button.pack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eeca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output text box\n",
    "output_text = tk.Text(window, height=10, width=60)\n",
    "output_text.pack(pady=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the summarizer pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Tkinter event loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcc896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6e9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41330e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341a907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7eab96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
